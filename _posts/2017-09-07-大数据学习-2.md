---
layout: post
title: 大数据学习-2
date:   2017-09-07 16:34:52 +0800
categories: BigData
---

* TOC
{:toc}

# 2 spark生态体系概念理解
Apache Spark是一种包含流处理能力的下一代批处理框架。与Hadoop的MapReduce引擎基于各种相同原则开发而来的Spark主要侧重于通过完善的内存计算和处理优化机制加快批处理工作负载的运行速度。

Spark可作为独立集群部署（需要相应存储层的配合），或可与Hadoop集成并取代MapReduce引擎。

* 批处理模式
与MapReduce不同，Spark的数据处理工作全部在内存中进行，只在一开始将数据读入内存，以及将最终结果持久存储时需要与存储层交互。所有中间态的处理结果均存储在内存中。

虽然内存中处理方式可大幅改善性能，Spark在处理与磁盘有关的任务时速度也有很大提升，因为通过提前对整个任务集进行分析可以实现更完善的整体式优化。为此Spark可创建代表所需执行的全部操作，需要操作的数据，以及操作和数据之间关系的Directed Acyclic Graph（有向无环图），即DAG，借此处理器可以对任务进行更智能的协调。

为了实现内存中批计算，Spark会使用一种名为Resilient Distributed Dataset（弹性分布式数据集），即RDD的模型来处理数据。这是一种代表数据集，只位于内存中，永恒不变的结构。针对RDD执行的操作可生成新的RDD。每个RDD可通过世系（Lineage）回溯至父级RDD，并最终回溯至磁盘上的数据。Spark可通过RDD在无需将每个操作的结果写回磁盘的前提下实现容错。

流处理模式
流处理能力是由Spark Streaming实现的。Spark本身在设计上主要面向批处理工作负载，为了弥补引擎设计和流处理工作负载特征方面的差异，Spark实现了一种叫做微批（Micro-batch）*的概念。在具体策略方面该技术可以将数据流视作一系列非常小的“批”，借此即可通过批处理引擎的原生语义进行处理。

Spark Streaming会以亚秒级增量对流进行缓冲，随后这些缓冲会作为小规模的固定数据集进行批处理。这种方式的实际效果非常好，但相比真正的流处理框架在性能方面依然存在不足。

## 2-1 spark 部署运行
## 2-2 spark程序开发
## 2-3 spark 编程模型RDD开发
## 2-4 作业执行解析
### 2-4-1 RDD视图 DAG图
### 2-4-2 sparkSQL DataFrame
## 2-5 Spark Streaming 
## 2-6 Spark MLib 与机器学习
## 2-7 GraphX SparkR 
## 2-8 Spark 项目实战
### 2-8-1 大数据分析系统
### 2-8-2 系统资源分析平台
### 2-8-3 在Spark上训练LR模型
### 2-8-4 获取二级邻居关系图

# 3 storm生态体系概念理解
//TODO

# 4 区别

* 仅批处理框架：
Apache Hadoop
* 仅流处理框架：
Apache Storm
Apache Samza
* 混合框架：
Apache Spark
Apache Flink



## 4-2